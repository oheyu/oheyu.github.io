<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>理解梯度下降：批量、随机与小批量的对比 | 史玉浩的个人博客</title>
<meta name="keywords" content="机器学习">
<meta name="description" content="在机器学习和深度学习中，梯度下降是一种广泛使用的优化算法，用于最小化模型的损失函数，从而优化模型参数。梯度下降的三种主要变体是批量梯度下降、">
<meta name="author" content="史玉浩">
<link rel="canonical" href="https://oheyu.github.io/zh/posts/tech/%E7%90%86%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E4%B8%8E%E5%B0%8F%E6%89%B9%E9%87%8F%E7%9A%84%E5%AF%B9%E6%AF%94/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.9c78afd72529c14d4d3c0fa583c3e82265e0d8a303d475223d3b37442260ba22.css" integrity="sha256-nHiv1yUpwU1NPA&#43;lg8PoImXg2KMD1HUiPTs3RCJguiI=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://oheyu.github.io/img/logo.svg">
<link rel="icon" type="image/png" sizes="16x16" href="https://oheyu.github.io/img/logo.svg">
<link rel="icon" type="image/png" sizes="32x32" href="https://oheyu.github.io/img/logo.svg">
<link rel="apple-touch-icon" href="https://oheyu.github.io/logo.svg">
<link rel="mask-icon" href="https://oheyu.github.io/logo.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://oheyu.github.io/zh/posts/tech/%E7%90%86%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E4%B8%8E%E5%B0%8F%E6%89%B9%E9%87%8F%E7%9A%84%E5%AF%B9%E6%AF%94/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="理解梯度下降：批量、随机与小批量的对比" />
<meta property="og:description" content="在机器学习和深度学习中，梯度下降是一种广泛使用的优化算法，用于最小化模型的损失函数，从而优化模型参数。梯度下降的三种主要变体是批量梯度下降、" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://oheyu.github.io/zh/posts/tech/%E7%90%86%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E4%B8%8E%E5%B0%8F%E6%89%B9%E9%87%8F%E7%9A%84%E5%AF%B9%E6%AF%94/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-10T15:39:18+08:00" />
<meta property="article:modified_time" content="2023-07-10T15:39:18+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="理解梯度下降：批量、随机与小批量的对比"/>
<meta name="twitter:description" content="在机器学习和深度学习中，梯度下降是一种广泛使用的优化算法，用于最小化模型的损失函数，从而优化模型参数。梯度下降的三种主要变体是批量梯度下降、"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "📚 文章",
      "item": "https://oheyu.github.io/zh/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "👨🏻‍💻 技术",
      "item": "https://oheyu.github.io/zh/posts/tech/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "理解梯度下降：批量、随机与小批量的对比",
      "item": "https://oheyu.github.io/zh/posts/tech/%E7%90%86%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E4%B8%8E%E5%B0%8F%E6%89%B9%E9%87%8F%E7%9A%84%E5%AF%B9%E6%AF%94/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "理解梯度下降：批量、随机与小批量的对比",
  "name": "理解梯度下降：批量、随机与小批量的对比",
  "description": "在机器学习和深度学习中，梯度下降是一种广泛使用的优化算法，用于最小化模型的损失函数，从而优化模型参数。梯度下降的三种主要变体是批量梯度下降、",
  "keywords": [
    "机器学习"
  ],
  "articleBody": "\r在机器学习和深度学习中，梯度下降是一种广泛使用的优化算法，用于最小化模型的损失函数，从而优化模型参数。梯度下降的三种主要变体是批量梯度下降、随机梯度下降和小批量梯度下降。本文将详细介绍这三种方法的定义、优缺点，以及它们在损失函数（准确度）曲线上的整体表现。\n一、批量梯度下降（Batch Gradient Descent） 1.1 定义 批量梯度下降（Batch Gradient Descent）在每次迭代时，使用整个训练集计算损失函数的梯度，并基于这个梯度来更新模型参数。损失函数 $J(\\theta)$ 通常是所有样本损失的平均值。例如，如果训练集中有 $N$ 个样本，则损失函数可以表示为：\n$$ J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} L(f(x^{(i)}; \\theta), y^{(i)})。 $$\n其中，$f(x^{(i)}; \\theta)$ 是模型的预测，$y^{(i)}$ 是第 $i$ 个样本的真实标签，$L$ 是损失函数（例如均方误差或交叉熵损失）。每次迭代的参数更新公式为：\n$$ \\theta := \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)。 $$\n1.2优点 稳定性高：每次参数更新使用整个训练集的梯度，梯度估计准确，更新稳定。 收敛性好：更容易接近全局最优解。 1.3 缺点 计算成本高：每次迭代都需要计算整个训练集的梯度，尤其对于大型数据集，计算成本非常高。 内存消耗大：需要在内存中存储整个训练集，内存消耗大。 二、随机梯度下降（Stochastic Gradient Descent, SGD） 2.1定义 随机梯度下降在每次迭代时，仅使用一个样本计算损失函数的梯度，并基于这个梯度来更新模型参数。损失函数 $J(\\theta; x^{(i)}, y^{(i)})$ 是第 $i$ 个样本的损失函数。每次迭代的参数更新公式为：\n$$ \\theta := \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)}) $$\n2.2优点 计算成本低：每次迭代只需计算一个样本的梯度，计算成本低。 快速更新：由于每次迭代计算量小，参数更新频繁，初期收敛速度快。 2.3 缺点 梯度估计噪声大：每次更新的梯度波动大，导致优化路径不稳定。 收敛性差：容易在局部最优解附近震荡，难以收敛到全局最优解。 三、小批量梯度下降（Mini-batch Gradient Descent） 3.1 定义 小批量梯度下降是批量梯度下降和随机梯度下降的折中方案。每次迭代时，使用一个小批量样本（mini-batch）计算梯度，并基于这个梯度来更新模型参数。损失函数 $J(\\theta; B)$ 是一个小批量 $B$ 样本的平均损失。每次迭代的参数更新公式为：\n$$\\theta := \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta; B)$$\n3.2 优点 计算效率高：相比批量梯度下降，每次迭代计算的小批量样本梯度减少了计算成本。 收敛速度快：相比随机梯度下降，更新更加稳定，减少了震荡。 硬件效率：可以利用 GPU 加速计算，提高计算效率。 3.3 缺点 需要调参：需要选择合适的小批量大小，影响性能。 复杂度增加：相对于随机梯度下降和批量梯度下降，算法实现复杂度增加。 四、三者之间的联系与区别 4.1 联系 目标一致：三种方法都旨在通过梯度下降优化损失函数，以找到模型参数的最优值。 基本原理相同：三种方法的核心都是基于梯度下降，即使用损失函数的梯度来更新模型参数。 4.2 区别 数据使用方式： 批量梯度下降：使用整个训练集计算梯度。 随机梯度下降：每次迭代只使用一个样本计算梯度。 小批量梯度下降：每次迭代使用一个小批量样本计算梯度。 计算成本： 批量梯度下降：计算成本最高，因为每次迭代计算整个训练集的梯度。 随机梯度下降：计算成本最低，因为每次迭代只计算一个样本的梯度。 小批量梯度下降：计算成本介于两者之间。 收敛速度与稳定性： 批量梯度下降：更新稳定，收敛速度较慢。 随机梯度下降：初期收敛速度快，但更新不稳定，容易震荡。 小批量梯度下降：折中了两者的优点，更新较稳定，收敛速度较快。 内存消耗： 批量梯度下降：需要在内存中存储整个训练集。 随机梯度下降：内存消耗最小，只需存储一个样本。 小批量梯度下降：内存消耗介于两者之间，只需存储一个小批量样本。 五、反映在损失函数曲线和准确度曲线上的表现 5.1 批量梯度下降 损失函数曲线：曲线平滑，稳步下降，趋向于全局最优解。 准确度曲线：曲线平滑，准确度稳步上升，最终趋于收敛。 5.2 随机梯度下降 损失函数曲线：曲线波动大，总体趋势下降，但存在震荡和噪声。 准确度曲线：曲线波动大，总体上升，但有时会明显下降。 5.3 小批量梯度下降 损失函数曲线：曲线相对平滑，但有一些波动，介于批量和随机之间。 准确度曲线：曲线相对平稳，但有一些波动，总体上升。 总结 批量梯度下降、随机梯度下降和小批量梯度下降是机器学习和深度学习中常见的优化算法。它们在计算成本、收敛速度和更新稳定性上各有优缺点。相比较批量梯度下降与随机梯度下降，小批量梯度下降通过结合两者的优点，广泛应用于实际深度学习任务中。\n",
  "wordCount" : "1786",
  "inLanguage": "zh",
  "datePublished": "2023-07-10T15:39:18+08:00",
  "dateModified": "2023-07-10T15:39:18+08:00",
  "author":[{
    "@type": "Person",
    "name": "史玉浩"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://oheyu.github.io/zh/posts/tech/%E7%90%86%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E4%B8%8E%E5%B0%8F%E6%89%B9%E9%87%8F%E7%9A%84%E5%AF%B9%E6%AF%94/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "史玉浩的个人博客",
    "logo": {
      "@type": "ImageObject",
      "url": "https://oheyu.github.io/img/logo.svg"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://oheyu.github.io/zh/" accesskey="h" title="史玉浩的个人博客 (Alt + H)">史玉浩的个人博客</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://oheyu.github.io/en/" title="English"
                            aria-label="English">English</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://oheyu.github.io/zh/search" title="🔍搜索 (Alt &#43; /)" accesskey=/>
                    <span>🔍搜索</span>
                </a>
            </li>
            <li>
                <a href="https://oheyu.github.io/zh/" title="🏠主页">
                    <span>🏠主页</span>
                </a>
            </li>
            <li>
                <a href="https://oheyu.github.io/zh/posts" title="📚文章">
                    <span>📚文章</span>
                </a>
            </li>
            <li>
                <a href="https://oheyu.github.io/zh/archives" title="⏱时间轴">
                    <span>⏱时间轴</span>
                </a>
            </li>
            <li>
                <a href="https://oheyu.github.io/zh/categories" title="🧩分类">
                    <span>🧩分类</span>
                </a>
            </li>
            <li>
                <a href="https://oheyu.github.io/zh/tags" title="🔖标签">
                    <span>🔖标签</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://oheyu.github.io/zh/">主页</a>&nbsp;»&nbsp;<a href="https://oheyu.github.io/zh/posts/">📚 文章</a>&nbsp;»&nbsp;<a href="https://oheyu.github.io/zh/posts/tech/">👨🏻‍💻 技术</a></div>
    <h1 class="post-title entry-hint-parent">
      理解梯度下降：批量、随机与小批量的对比
    </h1>
    <div class="post-meta"><span title='2023-07-10 15:39:18 +0800 CST'>2023-07-10</span>&nbsp;·&nbsp;4 分钟&nbsp;·&nbsp;史玉浩

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">目录</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e4%b8%80%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dbatch-gradient-descent" aria-label="一、批量梯度下降（Batch Gradient Descent）">一、批量梯度下降（Batch Gradient Descent）</a><ul>
                            
                    <li>
                        <a href="#11-%e5%ae%9a%e4%b9%89" aria-label="1.1 定义">1.1 定义</a></li>
                    <li>
                        <a href="#12%e4%bc%98%e7%82%b9" aria-label="1.2优点">1.2优点</a></li>
                    <li>
                        <a href="#13-%e7%bc%ba%e7%82%b9" aria-label="1.3 缺点">1.3 缺点</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e4%ba%8c%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dstochastic-gradient-descent-sgd" aria-label="二、随机梯度下降（Stochastic Gradient Descent, SGD）">二、随机梯度下降（Stochastic Gradient Descent, SGD）</a><ul>
                            
                    <li>
                        <a href="#21%e5%ae%9a%e4%b9%89" aria-label="2.1定义">2.1定义</a></li>
                    <li>
                        <a href="#22%e4%bc%98%e7%82%b9" aria-label="2.2优点">2.2优点</a></li>
                    <li>
                        <a href="#23-%e7%bc%ba%e7%82%b9" aria-label="2.3 缺点">2.3 缺点</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e4%b8%89%e5%b0%8f%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dmini-batch-gradient-descent" aria-label="三、小批量梯度下降（Mini-batch Gradient Descent）">三、小批量梯度下降（Mini-batch Gradient Descent）</a><ul>
                            
                    <li>
                        <a href="#31-%e5%ae%9a%e4%b9%89" aria-label="3.1 定义">3.1 定义</a></li>
                    <li>
                        <a href="#32-%e4%bc%98%e7%82%b9" aria-label="3.2 优点">3.2 优点</a></li>
                    <li>
                        <a href="#33-%e7%bc%ba%e7%82%b9" aria-label="3.3 缺点">3.3 缺点</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e5%9b%9b%e4%b8%89%e8%80%85%e4%b9%8b%e9%97%b4%e7%9a%84%e8%81%94%e7%b3%bb%e4%b8%8e%e5%8c%ba%e5%88%ab" aria-label="四、三者之间的联系与区别">四、三者之间的联系与区别</a><ul>
                            
                    <li>
                        <a href="#41-%e8%81%94%e7%b3%bb" aria-label="4.1 联系">4.1 联系</a></li>
                    <li>
                        <a href="#42-%e5%8c%ba%e5%88%ab" aria-label="4.2 区别">4.2 区别</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e4%ba%94%e5%8f%8d%e6%98%a0%e5%9c%a8%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e6%9b%b2%e7%ba%bf%e5%92%8c%e5%87%86%e7%a1%ae%e5%ba%a6%e6%9b%b2%e7%ba%bf%e4%b8%8a%e7%9a%84%e8%a1%a8%e7%8e%b0" aria-label="五、反映在损失函数曲线和准确度曲线上的表现">五、反映在损失函数曲线和准确度曲线上的表现</a><ul>
                            
                    <li>
                        <a href="#51-%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d" aria-label="5.1 批量梯度下降">5.1 批量梯度下降</a></li>
                    <li>
                        <a href="#52-%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d" aria-label="5.2 随机梯度下降">5.2 随机梯度下降</a></li>
                    <li>
                        <a href="#53-%e5%b0%8f%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d" aria-label="5.3 小批量梯度下降">5.3 小批量梯度下降</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e6%80%bb%e7%bb%93" aria-label="总结">总结</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
  <div class="post-content"><!-- more -->
<p>在机器学习和深度学习中，梯度下降是一种广泛使用的优化算法，用于最小化模型的损失函数，从而优化模型参数。梯度下降的三种主要变体是批量梯度下降、随机梯度下降和小批量梯度下降。本文将详细介绍这三种方法的定义、优缺点，以及它们在损失函数（准确度）曲线上的整体表现。</p>
<hr>
<h3 id="一批量梯度下降batch-gradient-descent">一、批量梯度下降（Batch Gradient Descent）<a hidden class="anchor" aria-hidden="true" href="#一批量梯度下降batch-gradient-descent">#</a></h3>
<h4 id="11-定义">1.1 定义<a hidden class="anchor" aria-hidden="true" href="#11-定义">#</a></h4>
<p>批量梯度下降（Batch Gradient Descent）在每次迭代时，使用整个训练集计算损失函数的梯度，并基于这个梯度来更新模型参数。损失函数 $J(\theta)$ 通常是所有样本损失的平均值。例如，如果训练集中有 $N$ 个样本，则损失函数可以表示为：</p>
<p>$$
J(\theta) = \frac{1}{N} \sum_{i=1}^{N} L(f(x^{(i)}; \theta), y^{(i)})。
$$</p>
<p>其中，$f(x^{(i)}; \theta)$ 是模型的预测，$y^{(i)}$ 是第 $i$ 个样本的真实标签，$L$ 是损失函数（例如均方误差或交叉熵损失）。每次迭代的参数更新公式为：</p>
<p>$$
\theta := \theta - \eta \cdot \nabla_\theta J(\theta)。
$$</p>
<h4 id="12优点">1.2优点<a hidden class="anchor" aria-hidden="true" href="#12优点">#</a></h4>
<ul>
<li><strong>稳定性高</strong>：每次参数更新使用整个训练集的梯度，梯度估计准确，更新稳定。</li>
<li><strong>收敛性好</strong>：更容易接近全局最优解。</li>
</ul>
<h4 id="13-缺点">1.3 缺点<a hidden class="anchor" aria-hidden="true" href="#13-缺点">#</a></h4>
<ul>
<li><strong>计算成本高</strong>：每次迭代都需要计算整个训练集的梯度，尤其对于大型数据集，计算成本非常高。</li>
<li><strong>内存消耗大</strong>：需要在内存中存储整个训练集，内存消耗大。</li>
</ul>
<hr>
<h3 id="二随机梯度下降stochastic-gradient-descent-sgd">二、随机梯度下降（Stochastic Gradient Descent, SGD）<a hidden class="anchor" aria-hidden="true" href="#二随机梯度下降stochastic-gradient-descent-sgd">#</a></h3>
<h4 id="21定义">2.1定义<a hidden class="anchor" aria-hidden="true" href="#21定义">#</a></h4>
<p>随机梯度下降在每次迭代时，仅使用一个样本计算损失函数的梯度，并基于这个梯度来更新模型参数。损失函数 $J(\theta; x^{(i)}, y^{(i)})$ 是第 $i$ 个样本的损失函数。每次迭代的参数更新公式为：</p>
<p>$$
\theta := \theta - \eta \cdot \nabla_\theta J(\theta; x^{(i)}, y^{(i)})
$$</p>
<h4 id="22优点">2.2优点<a hidden class="anchor" aria-hidden="true" href="#22优点">#</a></h4>
<ul>
<li><strong>计算成本低</strong>：每次迭代只需计算一个样本的梯度，计算成本低。</li>
<li><strong>快速更新</strong>：由于每次迭代计算量小，参数更新频繁，初期收敛速度快。</li>
</ul>
<h4 id="23-缺点">2.3 缺点<a hidden class="anchor" aria-hidden="true" href="#23-缺点">#</a></h4>
<ul>
<li><strong>梯度估计噪声大</strong>：每次更新的梯度波动大，导致优化路径不稳定。</li>
<li><strong>收敛性差</strong>：容易在局部最优解附近震荡，难以收敛到全局最优解。</li>
</ul>
<hr>
<h3 id="三小批量梯度下降mini-batch-gradient-descent">三、小批量梯度下降（Mini-batch Gradient Descent）<a hidden class="anchor" aria-hidden="true" href="#三小批量梯度下降mini-batch-gradient-descent">#</a></h3>
<h4 id="31-定义">3.1 定义<a hidden class="anchor" aria-hidden="true" href="#31-定义">#</a></h4>
<p>小批量梯度下降是批量梯度下降和随机梯度下降的折中方案。每次迭代时，使用一个小批量样本（mini-batch）计算梯度，并基于这个梯度来更新模型参数。损失函数 $J(\theta; B)$ 是一个小批量 $B$ 样本的平均损失。每次迭代的参数更新公式为：</p>
<p>$$\theta := \theta - \eta \cdot \nabla_\theta J(\theta; B)$$</p>
<h4 id="32-优点">3.2 优点<a hidden class="anchor" aria-hidden="true" href="#32-优点">#</a></h4>
<ul>
<li><strong>计算效率高</strong>：相比批量梯度下降，每次迭代计算的小批量样本梯度减少了计算成本。</li>
<li><strong>收敛速度快</strong>：相比随机梯度下降，更新更加稳定，减少了震荡。</li>
<li><strong>硬件效率</strong>：可以利用 GPU 加速计算，提高计算效率。</li>
</ul>
<h4 id="33-缺点">3.3 缺点<a hidden class="anchor" aria-hidden="true" href="#33-缺点">#</a></h4>
<ul>
<li><strong>需要调参</strong>：需要选择合适的小批量大小，影响性能。</li>
<li><strong>复杂度增加</strong>：相对于随机梯度下降和批量梯度下降，算法实现复杂度增加。</li>
</ul>
<hr>
<h3 id="四三者之间的联系与区别">四、三者之间的联系与区别<a hidden class="anchor" aria-hidden="true" href="#四三者之间的联系与区别">#</a></h3>
<h4 id="41-联系">4.1 联系<a hidden class="anchor" aria-hidden="true" href="#41-联系">#</a></h4>
<ol>
<li><strong>目标一致</strong>：三种方法都旨在通过梯度下降优化损失函数，以找到模型参数的最优值。</li>
<li><strong>基本原理相同</strong>：三种方法的核心都是基于梯度下降，即使用损失函数的梯度来更新模型参数。</li>
</ol>
<h4 id="42-区别">4.2 区别<a hidden class="anchor" aria-hidden="true" href="#42-区别">#</a></h4>
<ol>
<li><strong>数据使用方式</strong>：
<ul>
<li><strong>批量梯度下降</strong>：使用整个训练集计算梯度。</li>
<li><strong>随机梯度下降</strong>：每次迭代只使用一个样本计算梯度。</li>
<li><strong>小批量梯度下降</strong>：每次迭代使用一个小批量样本计算梯度。</li>
</ul>
</li>
<li><strong>计算成本</strong>：
<ul>
<li><strong>批量梯度下降</strong>：计算成本最高，因为每次迭代计算整个训练集的梯度。</li>
<li><strong>随机梯度下降</strong>：计算成本最低，因为每次迭代只计算一个样本的梯度。</li>
<li><strong>小批量梯度下降</strong>：计算成本介于两者之间。</li>
</ul>
</li>
<li><strong>收敛速度与稳定性</strong>：
<ul>
<li><strong>批量梯度下降</strong>：更新稳定，收敛速度较慢。</li>
<li><strong>随机梯度下降</strong>：初期收敛速度快，但更新不稳定，容易震荡。</li>
<li><strong>小批量梯度下降</strong>：折中了两者的优点，更新较稳定，收敛速度较快。</li>
</ul>
</li>
<li><strong>内存消耗</strong>：
<ul>
<li><strong>批量梯度下降</strong>：需要在内存中存储整个训练集。</li>
<li><strong>随机梯度下降</strong>：内存消耗最小，只需存储一个样本。</li>
<li><strong>小批量梯度下降</strong>：内存消耗介于两者之间，只需存储一个小批量样本。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="五反映在损失函数曲线和准确度曲线上的表现">五、反映在损失函数曲线和准确度曲线上的表现<a hidden class="anchor" aria-hidden="true" href="#五反映在损失函数曲线和准确度曲线上的表现">#</a></h3>
<h4 id="51-批量梯度下降">5.1 批量梯度下降<a hidden class="anchor" aria-hidden="true" href="#51-批量梯度下降">#</a></h4>
<ul>
<li><strong>损失函数曲线</strong>：曲线平滑，稳步下降，趋向于全局最优解。</li>
<li><strong>准确度曲线</strong>：曲线平滑，准确度稳步上升，最终趋于收敛。</li>
</ul>
<h4 id="52-随机梯度下降">5.2 随机梯度下降<a hidden class="anchor" aria-hidden="true" href="#52-随机梯度下降">#</a></h4>
<ul>
<li><strong>损失函数曲线</strong>：曲线波动大，总体趋势下降，但存在震荡和噪声。</li>
<li><strong>准确度曲线</strong>：曲线波动大，总体上升，但有时会明显下降。</li>
</ul>
<h4 id="53-小批量梯度下降">5.3 小批量梯度下降<a hidden class="anchor" aria-hidden="true" href="#53-小批量梯度下降">#</a></h4>
<ul>
<li><strong>损失函数曲线</strong>：曲线相对平滑，但有一些波动，介于批量和随机之间。</li>
<li><strong>准确度曲线</strong>：曲线相对平稳，但有一些波动，总体上升。</li>
</ul>
<hr>
<h3 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h3>
<p>批量梯度下降、随机梯度下降和小批量梯度下降是机器学习和深度学习中常见的优化算法。它们在计算成本、收敛速度和更新稳定性上各有优缺点。相比较批量梯度下降与随机梯度下降，小批量梯度下降通过结合两者的优点，广泛应用于实际深度学习任务中。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://oheyu.github.io/zh/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://oheyu.github.io/zh/posts/tech/%E7%90%86%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/">
    <span class="title">« 上一页</span>
    <br>
    <span>理解梯度下降在机器学习、深度学习和强化学习中的应用</span>
  </a>
  <a class="next" href="https://oheyu.github.io/zh/posts/tech/vim%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E5%88%A0%E9%99%A4%E6%8B%AC%E5%8F%B7%E9%87%8C%E7%9A%84%E5%86%85%E5%AE%B9/">
    <span class="title">下一页 »</span>
    <br>
    <span>Vim如何快速删除括号里的内容</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://oheyu.github.io/zh/">史玉浩的个人博客</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
